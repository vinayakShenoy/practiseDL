# -*- coding: utf-8 -*-
"""NeuralStyleTransfer1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DFlXNUQvrBkoG7Vqic1tJKkk4wuFmmO8
"""

#!pip install q keras==2.3.0

import keras
from google.colab import files
import tensorflow as tf
from PIL import Image
import numpy as np
from matplotlib import pyplot as plt
import datetime

uploaded = files.upload()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

model = tf.keras.applications.VGG19(include_top=False, weights='imagenet', input_shape=(224,224,3))
model.trainable = False

model.summary()

def get_style_op():
  style_layer_names = ["block1_conv1","block2_conv1", "block3_conv1", "block4_conv1", "block5_conv1"]
  style_layers = list(filter(lambda x: x.name in style_layer_names, model.layers))
  style_output_layers = [layer.output for layer in style_layers]
  style_op = tf.keras.models.Model(inputs=model.input, outputs=style_output_layers)
  return style_op

def get_content_op():
  content_layer_names = ["block3_conv1"]
  content_layers = list(filter(lambda x: x.name in content_layer_names, model.layers))
  content_output_layers = [layer.output for layer in content_layers]
  content_op = tf.keras.models.Model(inputs=model.input, outputs=content_output_layers)
  return content_op

img = tf.io.read_file("/content/style1.jpg")
img = tf.image.decode_jpeg(img, channels=3)
img = tf.image.convert_image_dtype(img, tf.float32)
img = tf.image.resize(img, [224, 224])
img = tf.reshape(img, [1,224,224,3])
reference_style_output = get_style_op()(img)

img = tf.io.read_file("/content/content1.jpg")
img = tf.image.decode_jpeg(img, channels=3)
img = tf.image.convert_image_dtype(img, tf.float32)
img = tf.image.resize(img, [224, 224])
img = tf.reshape(img, [1,224,224,3])
reference_content_output = get_content_op()(img)

img = tf.io.read_file("/content/content1.jpg")
img = tf.image.decode_jpeg(img, channels=3)
img = tf.image.convert_image_dtype(img, tf.float32)
img = tf.image.resize(img, [224, 224])
img = tf.reshape(img, [1,224,224,3])
#img1 = tf.random.uniform(shape=(1,224,224,3), minval=0, maxval=1)
training_img = tf.Variable(img)

def content_cost(train_img_content_output):
  content_cost = tf.reduce_sum(tf.square(tf.subtract(reference_content_output[0], train_img_content_output)))
  return content_cost

def total_style_cost(train_img_style_ouput):
  def layer_style_cost(layer_num, ref_layer_op):
    def gram_matrix(a):
      m, n_H, n_W, n_C = a.shape
      a = tf.transpose(a, perm=[0,3,1,2])
      a = tf.reshape(a,[m, n_C, n_H*n_W])
      g_matrix = tf.linalg.matmul(a, a, transpose_b=True)
      return g_matrix

    m, n_H, n_W, n_C = ref_layer_op.shape
    gram_matrix_train = gram_matrix(train_img_style_ouput[layer_num])
    #print(gram_matrix_train)
    gram_matrix_ref = gram_matrix(ref_layer_op)
    cost = tf.reduce_sum(tf.square(tf.subtract(gram_matrix_train,gram_matrix_ref)))/((2*n_C*n_H*n_W)**2)
    return cost

  total_cost = 0
  for (i, ref_layer_op) in enumerate(reference_style_output):
    total_cost += layer_style_cost(i,ref_layer_op)
  total_cost /= len(reference_style_output)
  return total_cost

style_weight=1
content_weight=1
def net_cost(training_img):
  tr_style_op = get_style_op()(training_img)
  ts = total_style_cost(tr_style_op)
  tr_content_op = get_content_op()(training_img)
  tc = content_cost(tr_content_op)
  nt = style_weight*ts + content_weight*tc
  return nt,ts,tc

def clip_0_1(image):
  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)

train_net_loss = tf.keras.metrics.Mean('train_net_loss', dtype=tf.float32)
train_style_loss = tf.keras.metrics.Mean('train_style_loss', dtype=tf.float32)
train_content_loss = tf.keras.metrics.Mean('train_content_loss', dtype=tf.float32)
#train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')

opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)
@tf.function()
def train_step(image):
  with tf.GradientTape() as tape:
    loss, ts, tc = net_cost(image)

  grad = tape.gradient(loss, image)
  opt.apply_gradients([(grad, image)])
  training_img.assign(clip_0_1(image))

  train_net_loss(loss)
  train_style_loss(ts)
  train_content_loss(tc)

current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
train_log_dir = 'logs/gradient_tape/' + current_time + '/train'
train_summary_writer = tf.summary.create_file_writer(train_log_dir)

def display_img(op_img):
  plt.imshow(op_img, interpolation='nearest')
  plt.show()

train_step(training_img)
train_step(training_img)
op_img = tf.reshape(training_img, [224,224,3]).numpy()
display_img(op_img)

epochs = 1000
steps_per_epoch = 100
step = 0
for n in range(epochs):
  for m in range(steps_per_epoch):
    step += 1
    train_step(training_img)
    print(".", end='')
  with train_summary_writer.as_default():
    tf.summary.scalar('net_loss', train_net_loss.result(), step=n)
    tf.summary.scalar('style_loss', train_style_loss.result(), step=n)
    tf.summary.scalar('content_loss', train_content_loss.result(), step=n)
  template = 'Epoch {}, Net Loss: {}, Style Loss: {}, Content Loss: {}'
  print (template.format(n+1,
                         train_net_loss.result(), train_style_loss.result(), train_content_loss.result()))
  train_net_loss.reset_states()
  train_style_loss.reset_states()
  train_content_loss.reset_states()
  display_img(op_img)

